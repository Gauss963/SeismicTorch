{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from sample_hbb.csv\n",
      "40000 events loaded.\n",
      "load from sample_q.csv\n",
      "40000 events loaded.\n",
      "load from sample_test.csv\n",
      "40000 events loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 1ms/step - loss: 0.6922 - accuracy: 0.5491\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6908 - accuracy: 0.5677\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6896 - accuracy: 0.6051\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6880 - accuracy: 0.5975\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6855 - accuracy: 0.6207\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6821 - accuracy: 0.5944\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6761 - accuracy: 0.6298\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6681 - accuracy: 0.6327\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6567 - accuracy: 0.6399\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6437 - accuracy: 0.6430\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6284 - accuracy: 0.6628\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6114 - accuracy: 0.6775\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5923 - accuracy: 0.7037\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5731 - accuracy: 0.7253\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5573 - accuracy: 0.7399\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5429 - accuracy: 0.7510\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7562\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5222 - accuracy: 0.7603\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5117 - accuracy: 0.7609\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5003 - accuracy: 0.7615\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4865 - accuracy: 0.7694\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4716 - accuracy: 0.7868\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4597 - accuracy: 0.7984\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4509 - accuracy: 0.8020\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4451 - accuracy: 0.8078\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4413 - accuracy: 0.8092\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4391 - accuracy: 0.8123\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4376 - accuracy: 0.8127\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4365 - accuracy: 0.8140\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 0.4352 - accuracy: 0.8149\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4349 - accuracy: 0.8156\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4345 - accuracy: 0.8159\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4340 - accuracy: 0.8158\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4334 - accuracy: 0.8167\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4332 - accuracy: 0.8166\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4330 - accuracy: 0.8159\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4328 - accuracy: 0.8166\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4322 - accuracy: 0.8171\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4326 - accuracy: 0.8170\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4318 - accuracy: 0.8174\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4322 - accuracy: 0.8172\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4321 - accuracy: 0.8174\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4321 - accuracy: 0.8170\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4313 - accuracy: 0.8177\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4318 - accuracy: 0.8174\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4319 - accuracy: 0.8172\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4317 - accuracy: 0.8172\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4313 - accuracy: 0.8175\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4314 - accuracy: 0.8171\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4311 - accuracy: 0.8176\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4314 - accuracy: 0.8180\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4309 - accuracy: 0.8176\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4313 - accuracy: 0.8178\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4309 - accuracy: 0.8179\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4311 - accuracy: 0.8179\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4308 - accuracy: 0.8182\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4312 - accuracy: 0.8178\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4305 - accuracy: 0.8177\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4310 - accuracy: 0.8178\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4327 - accuracy: 0.8172\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4310 - accuracy: 0.8177\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4307 - accuracy: 0.8184\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4308 - accuracy: 0.8179\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4304 - accuracy: 0.8183\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4308 - accuracy: 0.8178\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4305 - accuracy: 0.8184\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4307 - accuracy: 0.8176\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4301 - accuracy: 0.8183\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4306 - accuracy: 0.8181\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 0.4310 - accuracy: 0.8174\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4305 - accuracy: 0.8178\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4306 - accuracy: 0.8179\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4306 - accuracy: 0.8182\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4311 - accuracy: 0.8178\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4305 - accuracy: 0.8182\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4303 - accuracy: 0.8179\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4304 - accuracy: 0.8177\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4301 - accuracy: 0.8184\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4303 - accuracy: 0.8184\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 0.4299 - accuracy: 0.8186\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4303 - accuracy: 0.8182\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.4301 - accuracy: 0.8183\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4303 - accuracy: 0.8183\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.4309 - accuracy: 0.8176\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4302 - accuracy: 0.8185\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4307 - accuracy: 0.8180\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4300 - accuracy: 0.8185\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 0.4306 - accuracy: 0.8178\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4301 - accuracy: 0.8183\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 0.4297 - accuracy: 0.8188\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4300 - accuracy: 0.8183\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4306 - accuracy: 0.8179\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4299 - accuracy: 0.8189\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.4296 - accuracy: 0.8185\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4299 - accuracy: 0.8185\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4295 - accuracy: 0.8184\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4300 - accuracy: 0.8180\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.4294 - accuracy: 0.8188\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4299 - accuracy: 0.8183\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 0.4299 - accuracy: 0.8185\n",
      "2500/2500 [==============================] - 3s 1ms/step\n",
      "1250/1250 [==============================] - 2s 1ms/step\n",
      "(80000, 4)\n",
      "(80000,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from ann_visualizer.visualize import ann_viz;\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def load_samples(filename):\n",
    "    print('load from',filename)\n",
    "    fin = open(filename)\n",
    "    lines = fin.readlines()\n",
    "    idx_cur, samples, evt = -1, [], []\n",
    "    for l in lines[1:]:\n",
    "        idx, code, px, py, pz = l.split(',')\n",
    "        if idx_cur!=int(idx):\n",
    "            idx_cur=int(idx)\n",
    "            if len(evt)>0:\n",
    "                samples.append(evt)\n",
    "                evt = []\n",
    "        else:\n",
    "            evt.append([int(code),float(px),float(py),float(pz)])\n",
    "    samples.append(evt)\n",
    "    print(len(samples),'events loaded.')\n",
    "    return samples\n",
    "    \n",
    "def prepare_features(samples):\n",
    "    features = []\n",
    "    for evt in samples:\n",
    "        px = py = pz = E = 0.\n",
    "        count_id_zero = 0.\n",
    "        count_id_three_plus_four = 0.\n",
    "        for p in evt:\n",
    "            px += p[1]\n",
    "            py += p[2]\n",
    "            pz += p[3]\n",
    "            E += (p[1]**2+p[2]**2+p[3]**2)**0.5\n",
    "            if p[0] == 0:\n",
    "                count_id_zero += 1\n",
    "            if p[0] == 3 or 4 :\n",
    "                count_id_three_plus_four += 1\n",
    "        M = (E**2 - (px**2+py**2+pz**2))**0.5\n",
    "        features.append([len(evt), M, count_id_zero - 2, count_id_three_plus_four])\n",
    "    return features\n",
    "            \n",
    "sample_sig = load_samples('sample_hbb.csv')\n",
    "sample_bkg = load_samples('sample_q.csv')\n",
    "sample_test = load_samples('sample_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the features\n",
    "x_train = np.array(prepare_features(sample_sig) + prepare_features(sample_bkg))\n",
    "y_train = np.array([1]*len(sample_sig)+[0]*len(sample_bkg))\n",
    "x_test = np.array(prepare_features(sample_test))\n",
    "\n",
    "# Initialize the Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 7, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 10, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 5, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = SGD(lr = 0.05),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "num_of_epoch = 50\n",
    "# 創建一個列表來記錄每個epoch的精確度\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "# 訓練模型並記錄每個epoch的精確度\n",
    "for epoch in range(num_of_epoch):\n",
    "    history = model.fit(x_train, y_train, epochs = 1, batch_size = 80)\n",
    "    _, accuracy = model.evaluate(x_train, y_train)\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(history.history['loss'][0])\n",
    "\n",
    "f_train = model.predict(x_train)\n",
    "\n",
    "p_test = model.predict(x_test)\n",
    "fout = open('my_predicts.csv','w')\n",
    "fout.write(\"id,class\\n\")\n",
    "for idx,p in enumerate(p_test):\n",
    "    fout.write('%d,%d\\n' % (idx,p>0.5))\n",
    "fout.close()\n",
    "\n",
    "\n",
    "ann_viz(model, title = \"Higgs Boson Prediction NN Structure\", view = True, filename = 'My_NN_Structure')\n",
    "\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "\n",
    "\n",
    "font1 = {'family':'times new roman','color':'black','size':20}\n",
    "font2 = {'family':'times new roman','color':'black','size':15}\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "fig1.set_size_inches(8, 6)\n",
    "ax1.hist(x_train[y_train == 0][:,0],label = 'Light Quark' , bins=50, color='y')\n",
    "ax1.hist(x_train[y_train == 1][:,0],label = 'Higgs' , bins=50, color='g', alpha=0.5)\n",
    "ax1.set_xlabel(\"Number of Sets\", fontdict = font2)\n",
    "ax1.set_ylabel(\"Number of Particles\", fontdict = font2)\n",
    "ax1.legend(loc = 1)\n",
    "fig1.savefig('./Counting_of_Particles.pdf', dpi=900, bbox_inches='tight')\n",
    "plt.close(fig1)\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "fig2.set_size_inches(8, 6)\n",
    "ax2.hist(x_train[y_train == 0][:,1],label = 'Light Quark' , bins=50, color='y')\n",
    "ax2.hist(x_train[y_train == 1][:,1],label = 'Higgs' , bins=50, color='g', alpha=0.5)\n",
    "ax2.set_xlabel(\"Number of Particles\", fontdict = font2)\n",
    "ax2.set_ylabel(\"Invariant Mass\", fontdict = font2)\n",
    "ax2.legend(loc = 1)\n",
    "fig2.savefig('./Invariant_Mass.pdf', dpi=900, bbox_inches='tight')\n",
    "plt.close(fig2)\n",
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "fig3.set_size_inches(8, 6)\n",
    "ax3.hist(f_train[y_train == 0],label = 'Light Quark' , bins=50, color='y')\n",
    "ax3.hist(f_train[y_train == 1],label = 'Higgs' , bins=50, color='g', alpha=0.5)\n",
    "ax3.set_xlabel(\"Prediction\", fontdict = font2)\n",
    "ax3.set_ylabel(\"Number of Particles\", fontdict = font2)\n",
    "ax3.legend(loc = 1)\n",
    "fig3.savefig('./ANN_Output.pdf', dpi=900, bbox_inches='tight')\n",
    "plt.close(fig3)\n",
    "\n",
    "fig4, ax4 = plt.subplots()\n",
    "fig4.set_size_inches(8, 6)\n",
    "ax4.plot(range(1, num_of_epoch + 1), accuracy_list, label = 'Accuracy')\n",
    "ax4.plot(range(1, num_of_epoch + 1), loss_list, label = 'Loss')\n",
    "ax4.set_xlabel(\"Epoch\", fontdict = font2)\n",
    "ax4.set_ylabel(\"Accuracy\", fontdict = font2)\n",
    "ax4.legend(loc = 1)\n",
    "fig4.savefig('./Accuracy.pdf', dpi=900, bbox_inches='tight')\n",
    "plt.close(fig4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
