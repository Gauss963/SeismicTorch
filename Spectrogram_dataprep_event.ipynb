{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import obspy\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "from io import BytesIO\n",
    "from obspy import Stream\n",
    "from obspy import read\n",
    "from obspy.taup import TauPyModel\n",
    "from obspy.geodetics import gps2dist_azimuth\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "\n",
    "import numpy as np \n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_amplitude(data, rate):\n",
    "    'Scale amplitude or waveforms'\n",
    "    \n",
    "    tmp = np.random.uniform(0, 1)\n",
    "    if tmp < rate:\n",
    "        data *= np.random.uniform(1, 3)\n",
    "    elif tmp < 2 * rate:\n",
    "        data /= np.random.uniform(1, 3)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_event(data, maxshift, rate, start, halfdim): \n",
    "    'Randomly rotate the array to shift the event location'\n",
    "    \n",
    "    if np.random.uniform(0, 1) < rate:\n",
    "        start += int(np.random.uniform(-maxshift, maxshift))             \n",
    "    return data[:, start-halfdim:start + halfdim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wave_arrival(input_Stream):\n",
    "\n",
    "    # 選取第一個Trace對象\n",
    "    tr = input_Stream[0]\n",
    "\n",
    "    # 測站的緯度和經度\n",
    "    station_latitude = tr.stats.sac.stla\n",
    "    station_longitude = tr.stats.sac.stlo\n",
    "\n",
    "\n",
    "    # 地震源的緯度和經度\n",
    "    event_latitude = tr.stats.sac.evla\n",
    "    event_longitude = evlo = tr.stats.sac.evlo\n",
    "\n",
    "\n",
    "    # 使用 ObsPy 的 gps2dist_azimuth 函數來計算距離\n",
    "    distance_m, azimuth1, azimuth2 = gps2dist_azimuth(station_latitude, station_longitude, event_latitude, event_longitude)\n",
    "    distance_in_km = distance_m / 1000\n",
    "\n",
    "    # 將距離從公尺轉換為地震中心距離（以度為單位）\n",
    "    distance_in_degrees = distance_m / 1000 / 111.195\n",
    "\n",
    "\n",
    "    # 取得地震源的深度\n",
    "    depth_in_km = tr.stats.sac.evdp\n",
    "\n",
    "    # 總距離是地震中心距離與地震深度的和\n",
    "    total_distance_in_km = distance_in_km + depth_in_km\n",
    "\n",
    "    # 假定的P波和S波速度\n",
    "    p_wave_velocity = 6  # km/sec\n",
    "    s_wave_velocity = 3.5  # km/sec\n",
    "\n",
    "    # 使用模型來計算抵達時間\n",
    "    model = TauPyModel(model = \"iasp91\")\n",
    "    model = TauPyModel(model = \"ak135\")\n",
    "    arrivals = model.get_travel_times(source_depth_in_km = depth_in_km, distance_in_degree = distance_in_degrees)\n",
    "\n",
    "    # 尋找P波和S波的抵達時間\n",
    "    P_arrival_time = None\n",
    "    S_arrival_time = None\n",
    "    for arrival in arrivals:\n",
    "        if arrival.name == 'Pn':\n",
    "            P_arrival_time = arrival.time\n",
    "        elif arrival.name == 'Sn':\n",
    "            S_arrival_time = arrival.time\n",
    "        elif arrival.name == 'P':\n",
    "            P_arrival_time = arrival.time\n",
    "        elif arrival.name == 'S':\n",
    "            S_arrival_time = arrival.time\n",
    "\n",
    "    # 如果模型無法提供P波或S波的抵達時間，則使用簡單的估計方法\n",
    "    if P_arrival_time is None:\n",
    "        # print('simple estimation')\n",
    "        P_arrival_time = total_distance_in_km / p_wave_velocity\n",
    "\n",
    "    if S_arrival_time is None:\n",
    "        # print('simple estimation')\n",
    "        S_arrival_time = total_distance_in_km / s_wave_velocity\n",
    "\n",
    "    # print('Estimated P wave arrival time: ', P_arrival_time, 'seconds')\n",
    "    # print('Estimated S wave arrival time: ', S_arrival_time, 'seconds')\n",
    "\n",
    "    P_arrival_sample = P_arrival_time * 100\n",
    "    S_arrival_sample = S_arrival_time * 100\n",
    "\n",
    "    P_arrival_sample = int(P_arrival_sample)\n",
    "    S_arrival_sample = int(S_arrival_sample)\n",
    "\n",
    "    return P_arrival_sample, S_arrival_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger_sample(input_Stream, p_start_abs, s_start_abs):\n",
    "\n",
    "    p_max_abs = p_start_abs\n",
    "    s_max_abs = s_start_abs\n",
    "\n",
    "    p_wave_window = 12  # 12 sec\n",
    "    s_wave_window = 12  # 12 sec\n",
    "\n",
    "    spacing_window =  1  # 1 sec\n",
    "\n",
    "    sampling_rate = input_Stream[0].stats.sampling_rate\n",
    "\n",
    "    # Calculate the composite acceleration\n",
    "    comp_acc = np.sqrt(input_Stream[0].data**2 + input_Stream[1].data**2 + input_Stream[2].data**2)\n",
    "    \n",
    "    # Ensure P-wave window doesn't extend into S-wave window\n",
    "    if p_start_abs + p_wave_window * sampling_rate >= s_start_abs - spacing_window * sampling_rate:\n",
    "        p_end_abs = s_start_abs - spacing_window * sampling_rate\n",
    "\n",
    "        s_end_abs = s_start_abs + s_wave_window * sampling_rate\n",
    "    else:\n",
    "        p_end_abs = p_start_abs + p_wave_window * sampling_rate\n",
    "        s_end_abs = s_start_abs + s_wave_window * sampling_rate\n",
    "\n",
    "    p_start_abs = int(p_start_abs)\n",
    "    s_start_abs = int(s_start_abs)\n",
    "    \n",
    "    p_end_abs = int(p_end_abs)\n",
    "    s_end_abs = int(s_end_abs)\n",
    "\n",
    "    p_window_samples = comp_acc[p_start_abs:p_end_abs]\n",
    "    s_window_samples = comp_acc[s_start_abs:s_end_abs]\n",
    "    \n",
    "    if len(p_window_samples) > 0:\n",
    "        p_max_sample = np.argmax(p_window_samples)\n",
    "        p_max_abs = max(p_max_abs, p_start_abs + p_max_sample)\n",
    "    \n",
    "    if len(s_window_samples) > 0:\n",
    "        s_max_sample = np.argmax(s_window_samples)\n",
    "        s_max_abs = max(s_max_abs, s_start_abs + s_max_sample)\n",
    "\n",
    "    return p_max_abs, s_max_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obspy_stream(input_Stream):\n",
    "\n",
    "    tr = input_Stream[0]\n",
    "\n",
    "\n",
    "    fsin = 100.0\n",
    "    fsout = 40.0\n",
    "    wlen = 2.0\n",
    "    alpha = 0.05\n",
    "    freq = 0.5\n",
    "    maxshift = 80\n",
    "    shift_event_r = 0.995\n",
    "    # shift_event_r = 0.0\n",
    "    scale_amplitude_r = 0.3\n",
    "    # scale_amplitude_r = 0.0\n",
    "\n",
    "    dim = int(wlen * fsout)\n",
    "    indim = int(fsin * wlen) // 2 \n",
    "\n",
    "\n",
    "    # Apply Filter to Stream\n",
    "    input_Stream.detrend(type = \"linear\")\n",
    "    input_Stream.taper(alpha)\n",
    "    input_Stream.filter(type = \"highpass\", freq = freq)\n",
    "\n",
    "    # Get the wave arrivals\n",
    "    p_start_relative, s_start_relative = get_wave_arrival(input_Stream)\n",
    "    \n",
    "    trigger_time = tr.stats.sac.b * -1\n",
    "    trigger_sample = trigger_time * 100\n",
    "    trigger_sample = int(trigger_sample)\n",
    "\n",
    "    p_start_abs = trigger_sample + p_start_relative # +  6 * 100\n",
    "    s_start_abs = trigger_sample + s_start_relative + 12 * 100\n",
    "\n",
    "\n",
    "    # Get the maximum amplitude of waves\n",
    "    p_max_abs, s_max_abs = get_trigger_sample(input_Stream, p_start_abs, s_start_abs)\n",
    "\n",
    "    # Split the data\n",
    "    data = np.vstack([input_Stream[ii].data for ii in range(len(input_Stream))])\n",
    "    data1 = shift_event(data, maxshift, shift_event_r, p_max_abs, indim)\n",
    "    data2 = shift_event(data, maxshift, shift_event_r, s_max_abs, indim)\n",
    "\n",
    "    # Resample data\n",
    "    data1 = signal.resample(data1, dim, axis = -1)\n",
    "    data2 = signal.resample(data2, dim, axis = -1)\n",
    "\n",
    "    # Scale data amplitude\n",
    "    if scale_amplitude_r:\n",
    "        data1 = scale_amplitude(data1, scale_amplitude_r)\n",
    "        data2 = scale_amplitude(data2, scale_amplitude_r)\n",
    "\n",
    "    # Transpose data\n",
    "    data1 = data1.transpose()\n",
    "    data2 = data2.transpose()\n",
    "\n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_to_spectrogram_ndarray(input_Stream):\n",
    "    xyz = ['x', 'y', 'z']\n",
    "    array_list = []\n",
    "    for i in range(3):\n",
    "        trace = input_Stream[i]\n",
    "        trace_acceleration = trace.data\n",
    "\n",
    "        # 設置 Spectrogram 參數\n",
    "        fs = trace.stats.sampling_rate  # 取樣率\n",
    "        nperseg = 256                   # 每個段的數據點數\n",
    "        noverlap = nperseg // 2         # 重疊的數據點數\n",
    "\n",
    "        # Draw Spectrogram\n",
    "        frequencies, times, Sxx = scipy.signal.spectrogram(trace_acceleration, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "        plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx), shading = 'auto', cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Use BytesIO to save Matplotlib image to ram\n",
    "        img_stream = BytesIO()\n",
    "        plt.savefig(img_stream, format = 'png', bbox_inches = 'tight', pad_inches = 0)\n",
    "        img_stream.seek(0)\n",
    "\n",
    "        # 使用 OpenCV 讀取並縮放圖片\n",
    "        img = cv2.imdecode(np.frombuffer(img_stream.read(), dtype=np.uint8), 1)\n",
    "        img_resized = cv2.resize(img, (150, 100))\n",
    "\n",
    "        # 將縮放後的圖片轉換為 NumPy 陣列\n",
    "        img_resized_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "        img_resized_gray_array = np.asarray(img_resized_gray)\n",
    "\n",
    "        cv2.imwrite('./Spectrogram_test/' + xyz[i] + '.png', img_resized)\n",
    "\n",
    "        array_list.append(img_resized_gray_array)\n",
    "\n",
    "    # 合併成一張彩色圖片 (100, 150, 3)\n",
    "    # Also, ues RGB instead of BGR. Easier to read\n",
    "    color_image = np.stack([array_list[2], array_list[0], array_list[1]], axis = -1)\n",
    "\n",
    "    return color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCEC = ['5AF48', '5AE11', '5ADF9', '5AE28', '5AF0F', '5AE83', '5AEBA', '5AFE5', '5AE1C']\n",
    "IES = ['5AE21', '5AE99', '5AE73', '5AF8A', '5AFA8', '5AEE6']\n",
    "\n",
    "station_list = RCEC + IES\n",
    "\n",
    "stream_list = []\n",
    "QSIS_train_data_X = np.array([])\n",
    "Spectrogram_train_data_list = []\n",
    "\n",
    "minimum_pga = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_part = 'HLX'\n",
    "\n",
    "all = 0\n",
    "useable = 0\n",
    "\n",
    "for datadir in station_list:\n",
    "\n",
    "    datadir = os.path.join('./data_QSIS_Event', datadir)\n",
    "\n",
    "    sac_files_X = glob.glob(f'{datadir}/*{file_name_part}*.sac')\n",
    "    sac_files_Y = [s.replace('X', 'Y') for s in sac_files_X]\n",
    "    sac_files_Z = [s.replace('X', 'Z') for s in sac_files_X]\n",
    "\n",
    "\n",
    "    for i in range(len(sac_files_X)):\n",
    "        stream = Stream()\n",
    "\n",
    "        stream_x = read(sac_files_X[i])\n",
    "        stream_x[0].data = stream_x[0].data * 0.01\n",
    "        stream += stream_x\n",
    "\n",
    "        stream_y = read(sac_files_Y[i])\n",
    "        stream_y[0].data = stream_y[0].data * 0.01\n",
    "        stream += stream_y\n",
    "\n",
    "        stream_z = read(sac_files_Z[i])\n",
    "        stream_z[0].data = stream_z[0].data * 0.01\n",
    "        stream += stream_z\n",
    "        \n",
    "        trace_x = stream_x[0]\n",
    "        trace_y = stream_y[0]\n",
    "        trace_z = stream_z[0]\n",
    "        \n",
    "\n",
    "        data_len_X = len(stream[0].data)\n",
    "        data_len_Y = len(stream[1].data)\n",
    "        data_len_Z = len(stream[2].data)\n",
    "        include_stream = False\n",
    "        include_stream = data_len_X == data_len_Y and data_len_Y == data_len_Z and data_len_Z == data_len_X\n",
    "        include_stream = include_stream and data_len_X >= 40000\n",
    "\n",
    "\n",
    "        if include_stream:\n",
    "            all = all + 1\n",
    "            pga_xyz = np.sqrt(trace_x.data**2 + trace_y.data**2 + trace_z.data**2)\n",
    "            pga_total = max(pga_xyz)\n",
    "            pga_total = pga_total * 100\n",
    "\n",
    "            # print(\"Total PGA (gal): \", pga_total)\n",
    "            if pga_total >= minimum_pga:\n",
    "                useable = useable + 1\n",
    "\n",
    "                stream_list.append(stream)\n",
    "\n",
    "print(useable)\n",
    "print(all)\n",
    "\n",
    "print(type(stream_list[0]))\n",
    "print(type(stream_list[0][0]))\n",
    "print(type(stream_list[0][1]))\n",
    "print(type(stream_list[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(stream_list)):\n",
    "#     # print(stream_list[i])\n",
    "#     p_data, s_data = process_obspy_stream(stream_list[i])\n",
    "#     QSIS_train_data_list.append(p_data)\n",
    "#     QSIS_train_data_list.append(s_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stream_list)):\n",
    "    spectrogram_data = stream_to_spectrogram_ndarray(stream_list[i])\n",
    "    Spectrogram_train_data_list.append(spectrogram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QSIS_train_data_X = np.stack(QSIS_train_data_list)\n",
    "# QSIS_train_data_X = np.expand_dims(QSIS_train_data_X, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_train_data_X = np.stack(spectrogram_data)\n",
    "spectrogram_train_data_X = np.expand_dims(spectrogram_train_data_X, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_array = np.array([[1., 0., 0.], [0., 1., 0.]])\n",
    "# QSIS_train_data_Y = np.tile(initial_array, (len(stream_list), 1))\n",
    "\n",
    "# print(QSIS_train_data_X.shape)\n",
    "# print(QSIS_train_data_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_initial_array = np.array([[1., 0.]])\n",
    "spectrogram_train_data_Y = np.tile(spectrogram_initial_array, (len(stream_list), 1))\n",
    "\n",
    "print(spectrogram_train_data_X.shape)\n",
    "print(spectrogram_train_data_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'Xevent': QSIS_train_data_X, 'Yevent': QSIS_train_data_Y}, './Spectrogram_data_pth/Spectrogram_event.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'Xevent': spectrogram_train_data_X, 'Yevent': spectrogram_train_data_Y}, './Spectrogram_data_pth/Spectrogram_event.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = './Spectrogram_data_pth'\n",
    "\n",
    "train_data_path = os.path.join(datadir, 'Spectrogram_event.pth')\n",
    "train_data = torch.load(train_data_path)\n",
    "Xevent = train_data['Xevent']\n",
    "Yevent = train_data['Yevent']\n",
    "\n",
    "print(Xevent.shape)\n",
    "print(Yevent.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
