{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Spectrogram_def_model import SpectrogramCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthquakeDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = './Spectrogram_data_pth'\n",
    "\n",
    "train_data_path = os.path.join(datadir, \"Spectrogram_all_merged.pth\")\n",
    "train_data = torch.load(train_data_path)\n",
    "Xall = train_data['Xall']\n",
    "Yall = train_data['Yall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xall, Yall = functions.remove_small_amplitude(Xall, Yall) \n",
    "# Xall, Yall = functions.remove_large_amplitude(Xall, Yall)\n",
    "# log10_amplitude = np.log10(np.max(np.abs(Xall), axis = (1, 2, 3)))\n",
    "# Xall = functions.normalize(Xall)\n",
    "# Xall = functions.quantize(Xall)\n",
    "# Xall = Xall.astype(np.float32)\n",
    "# Xall = functions.normalize(Xall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall = torch.from_numpy(Xall).float()\n",
    "Yall = torch.from_numpy(Yall).float()\n",
    "\n",
    "Xall = Xall.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation datasets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(Xall, Yall, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# create datasets\n",
    "train_dataset = EarthquakeDataset(X_train, Y_train)\n",
    "val_dataset = EarthquakeDataset(X_val, Y_val)\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 256, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 256, shuffle = True)\n",
    "\n",
    "# put dataloaders into a dictionary\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}\n",
    "\n",
    "# similarly, create a dictionary for dataset sizes\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'val': len(val_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, model, epoch_arr, train_loss_arr, val_loss_arr, train_acc_arr, val_acc_arr, lr_arr, cm, best_acc):\n",
    "        self.model = model\n",
    "        self.epoch_arr = epoch_arr\n",
    "        self.train_loss_arr = train_loss_arr\n",
    "        self.val_loss_arr = val_loss_arr\n",
    "        self.train_acc_arr = train_acc_arr\n",
    "        self.val_acc_arr = val_acc_arr\n",
    "        self.lr_arr = lr_arr\n",
    "        self.cm = cm\n",
    "        self.best_acc = best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_with_MyModel(MyModel_here, plot_title = \"Default\"):\n",
    "    epoch_arr_here = MyModel_here.epoch_arr\n",
    "    train_loss_arr_here = MyModel_here.train_loss_arr\n",
    "    val_loss_arr_here = MyModel_here.val_loss_arr\n",
    "    train_acc_arr_here = MyModel_here.train_acc_arr\n",
    "    val_acc_arr_here = MyModel_here.val_acc_arr\n",
    "    lr_arr_here = MyModel_here.lr_arr\n",
    "\n",
    "    fig_loss_func, axes_loss_func = plt.subplots(nrows = 1, ncols = 3, figsize = (24, 6))\n",
    "    fig_loss_func.suptitle(plot_title)\n",
    "\n",
    "    axes_loss_func[0].plot(epoch_arr_here, train_loss_arr_here, label = 'Training Loss')\n",
    "    axes_loss_func[0].plot(epoch_arr_here, val_loss_arr_here, label = 'Validation Loss')\n",
    "    axes_loss_func[0].set_title('Loss')\n",
    "    axes_loss_func[0].set_xlabel('Epoch')\n",
    "    axes_loss_func[0].set_ylabel('Loss')\n",
    "    axes_loss_func[0].legend()\n",
    "    axes_loss_func[1].plot(epoch_arr_here, train_acc_arr_here, label = 'Training Accuracy')\n",
    "    axes_loss_func[1].plot(epoch_arr_here, val_acc_arr_here, label = 'Validation Accuracy')\n",
    "    axes_loss_func[1].set_title('Accuracy')\n",
    "    axes_loss_func[1].set_xlabel('Epoch')\n",
    "    axes_loss_func[1].set_ylabel('Accurracy')\n",
    "    axes_loss_func[1].legend()\n",
    "    axes_loss_func[2].plot(epoch_arr_here, lr_arr_here, label = 'Learning Rate')\n",
    "    axes_loss_func[2].set_title('Learning Rate')\n",
    "    axes_loss_func[2].set_xlabel('Epoch')\n",
    "    axes_loss_func[2].set_ylabel('Accuracy')\n",
    "    axes_loss_func[2].legend()\n",
    "    fig_loss_func.tight_layout()\n",
    "    plt.savefig('./Spectrogram_Result/Spectrogram_Accurracy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm_with_MyModel(MyModel_here, plot_title = \"Default\"):\n",
    "\n",
    "    cm_here = MyModel_here.cm\n",
    "    cm_here = cm_here.astype('float') / cm_here.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "    # labels = ['P-wave', 'S-wave', 'Noise']\n",
    "    labels = ['Event', 'Noise']\n",
    "    cm_here_df = pd.DataFrame(cm_here, index = labels, columns = labels)\n",
    "    cm_here = cm_here_df\n",
    "\n",
    "    fig_cm_func, axes_cm_func = plt.subplots(nrows = 1, ncols = 1, figsize = (8, 6))\n",
    "    acc_here = MyModel_here.best_acc.detach().cpu().numpy() * 100\n",
    "    acc_here = round(acc_here, 2)\n",
    "    fig_cm_func.suptitle(plot_title + ' Confusion Matrix, Accuracy = ' + str(acc_here) + '%')\n",
    "\n",
    "    sns.heatmap(cm_here, annot = True, cmap = 'Blues', ax = axes_cm_func)\n",
    "    axes_cm_func.set_title('Train Confusion Matrix')\n",
    "    axes_cm_func.set_xlabel('Predicted Labels')\n",
    "    axes_cm_func.set_ylabel('Actual Labels')\n",
    "    fig_cm_func.tight_layout()\n",
    "\n",
    "    plt.savefig('./Spectrogram_Result/Spectrogram_CM.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs = 25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    epoch_arr = np.array([])\n",
    "    lr_arr = np.array([])\n",
    "    train_loss_arr = np.array([])\n",
    "    train_acc_arr = np.array([])\n",
    "\n",
    "    val_loss_arr = np.array([])\n",
    "    val_acc_arr = np.array([])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # These lists will hold all labels and predictions for this epoch\n",
    "            # all_labels = []\n",
    "            # all_preds = []\n",
    "\n",
    "            all_labels = np.array([])\n",
    "            all_preds = np.array([])\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs\n",
    "                labels = labels\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    _, labels = torch.max(labels.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Add current labels and predictions to the lists\n",
    "                # all_labels.extend(labels.tolist())\n",
    "                # all_preds.extend(preds.tolist())\n",
    "\n",
    "                all_labels = np.append(all_labels, labels.tolist())\n",
    "                all_preds = np.append(all_preds, preds.tolist())\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # After each epoch, compute and print the confusion matrix\n",
    "            if phase == 'val':\n",
    "                print(\"Confusion Matrix:\")\n",
    "                cm_here = confusion_matrix(all_labels, all_preds)\n",
    "                print(cm_here)\n",
    "\n",
    "            # save the loss and accurracy\n",
    "            if phase == 'train':\n",
    "                train_loss_arr = np.append(train_loss_arr, epoch_loss)\n",
    "                train_acc_arr = np.append(train_acc_arr, epoch_acc.detach().cpu())\n",
    "                lr_arr = np.append(lr_arr, optimizer.param_groups[0]['lr'])\n",
    "                epoch_arr = np.append(epoch_arr, epoch)\n",
    "            elif phase == 'val':\n",
    "                val_loss_arr = np.append(val_loss_arr, epoch_loss)\n",
    "                val_acc_arr = np.append(val_acc_arr, epoch_acc.detach().cpu())\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    model_class = MyModel(model = model, \n",
    "                          epoch_arr = epoch_arr, \n",
    "                          train_loss_arr = train_loss_arr, \n",
    "                          val_loss_arr = val_loss_arr, \n",
    "                          train_acc_arr = train_acc_arr, \n",
    "                          val_acc_arr = val_acc_arr, \n",
    "                          lr_arr = lr_arr, \n",
    "                          cm = cm_here, \n",
    "                          best_acc = best_acc)\n",
    "\n",
    "    return model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectrogramCNN()\n",
    "\n",
    "# 凍結模型的卷積層\n",
    "for param in model.parameters():\n",
    "    # param.requires_grad = False\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 全連接層的權重將被更新\n",
    "for param in model.fc1.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.fc2.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.fc3.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 使用Adam優化器，並且設定學習率為0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
    "# optimizer = optim.RAdam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# 每7個epoch，學習率減半\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_MyModel = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_with_MyModel(trained_MyModel, plot_title = \"CNN Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm_with_MyModel(MyModel_here = trained_MyModel, plot_title = \"CNN Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_MyModel.model, './model/SpectrogramCNN_trained.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
